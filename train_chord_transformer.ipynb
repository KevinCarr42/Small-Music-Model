{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a86692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a2401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training and validation data (encoded data)\n",
    "train_data = torch.tensor(np.fromfile('train.bin', dtype=np.int16), dtype=torch.long)\n",
    "val_data = torch.tensor(np.fromfile('val.bin', dtype=np.int16), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae805c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd69f9df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.216384 M parameters\n",
      "step 0: train loss 5.0620, val loss 5.0573\n",
      "step 50: train loss 1.9504, val loss 1.9169\n",
      "step 100: train loss 1.8541, val loss 1.8315\n",
      "step 150: train loss 1.7959, val loss 1.7812\n",
      "step 200: train loss 1.7571, val loss 1.7519\n",
      "step 250: train loss 1.7276, val loss 1.7228\n",
      "step 300: train loss 1.6996, val loss 1.7089\n",
      "step 350: train loss 1.6800, val loss 1.6869\n",
      "step 400: train loss 1.6615, val loss 1.6746\n",
      "step 450: train loss 1.6485, val loss 1.6587\n",
      "step 500: train loss 1.6363, val loss 1.6529\n",
      "step 550: train loss 1.6230, val loss 1.6383\n",
      "step 600: train loss 1.6121, val loss 1.6320\n",
      "step 650: train loss 1.6059, val loss 1.6295\n",
      "step 700: train loss 1.5969, val loss 1.6286\n",
      "step 750: train loss 1.5904, val loss 1.6280\n",
      "step 800: train loss 1.5869, val loss 1.6250\n",
      "step 850: train loss 1.5836, val loss 1.6166\n",
      "step 900: train loss 1.5767, val loss 1.6149\n",
      "step 950: train loss 1.5771, val loss 1.6135\n",
      "step 1000: train loss 1.5686, val loss 1.6140\n",
      "step 1050: train loss 1.5653, val loss 1.6092\n",
      "step 1100: train loss 1.5594, val loss 1.6089\n",
      "step 1150: train loss 1.5555, val loss 1.6094\n",
      "step 1200: train loss 1.5543, val loss 1.6058\n",
      "step 1250: train loss 1.5470, val loss 1.6039\n",
      "step 1300: train loss 1.5453, val loss 1.6055\n",
      "step 1350: train loss 1.5426, val loss 1.6035\n",
      "step 1400: train loss 1.5421, val loss 1.6025\n",
      "step 1450: train loss 1.5363, val loss 1.6019\n",
      "step 1500: train loss 1.5353, val loss 1.5980\n",
      "step 1550: train loss 1.5277, val loss 1.5967\n",
      "step 1600: train loss 1.5292, val loss 1.5969\n",
      "step 1650: train loss 1.5281, val loss 1.5966\n",
      "step 1700: train loss 1.5198, val loss 1.5988\n",
      "step 1750: train loss 1.5179, val loss 1.6027\n",
      "step 1800: train loss 1.5148, val loss 1.6051\n",
      "step 1850: train loss 1.5104, val loss 1.5944\n",
      "step 1900: train loss 1.5127, val loss 1.5991\n",
      "step 1950: train loss 1.5073, val loss 1.5967\n",
      "step 2000: train loss 1.5028, val loss 1.5921\n",
      "step 2050: train loss 1.5045, val loss 1.5996\n",
      "step 2100: train loss 1.4981, val loss 1.5941\n",
      "step 2150: train loss 1.4939, val loss 1.5957\n",
      "step 2200: train loss 1.4985, val loss 1.5940\n",
      "step 2250: train loss 1.4930, val loss 1.5986\n",
      "step 2300: train loss 1.4923, val loss 1.6041\n",
      "step 2350: train loss 1.4858, val loss 1.5938\n",
      "step 2400: train loss 1.4850, val loss 1.5956\n",
      "step 2450: train loss 1.4802, val loss 1.6034\n",
      "step 2500: train loss 1.4793, val loss 1.6005\n",
      "step 2550: train loss 1.4804, val loss 1.6017\n",
      "step 2600: train loss 1.4789, val loss 1.6048\n",
      "step 2650: train loss 1.4732, val loss 1.5950\n",
      "step 2700: train loss 1.4735, val loss 1.5978\n",
      "step 2750: train loss 1.4711, val loss 1.5980\n",
      "step 2800: train loss 1.4651, val loss 1.5998\n",
      "step 2850: train loss 1.4674, val loss 1.6060\n",
      "step 2900: train loss 1.4638, val loss 1.6018\n",
      "step 2950: train loss 1.4593, val loss 1.6094\n",
      "step 3000: train loss 1.4581, val loss 1.6037\n",
      "step 3050: train loss 1.4569, val loss 1.6085\n",
      "step 3100: train loss 1.4520, val loss 1.6087\n",
      "step 3150: train loss 1.4525, val loss 1.6096\n",
      "step 3200: train loss 1.4472, val loss 1.6099\n",
      "step 3250: train loss 1.4487, val loss 1.6202\n",
      "step 3300: train loss 1.4457, val loss 1.6116\n",
      "step 3350: train loss 1.4430, val loss 1.6122\n",
      "step 3400: train loss 1.4358, val loss 1.6087\n",
      "step 3450: train loss 1.4346, val loss 1.6195\n",
      "step 3500: train loss 1.4296, val loss 1.6192\n",
      "step 3550: train loss 1.4316, val loss 1.6165\n",
      "step 3600: train loss 1.4276, val loss 1.6227\n",
      "step 3650: train loss 1.4233, val loss 1.6261\n",
      "step 3700: train loss 1.4239, val loss 1.6248\n",
      "step 3750: train loss 1.4218, val loss 1.6156\n",
      "step 3800: train loss 1.4183, val loss 1.6210\n",
      "step 3850: train loss 1.4146, val loss 1.6185\n",
      "step 3900: train loss 1.4160, val loss 1.6243\n",
      "step 3950: train loss 1.4069, val loss 1.6264\n",
      "step 4000: train loss 1.4097, val loss 1.6271\n",
      "step 4050: train loss 1.4058, val loss 1.6384\n",
      "step 4100: train loss 1.3997, val loss 1.6332\n",
      "step 4150: train loss 1.4031, val loss 1.6305\n",
      "step 4200: train loss 1.3952, val loss 1.6332\n",
      "step 4250: train loss 1.3917, val loss 1.6302\n",
      "step 4300: train loss 1.3920, val loss 1.6307\n",
      "step 4350: train loss 1.3879, val loss 1.6374\n",
      "step 4400: train loss 1.3863, val loss 1.6400\n",
      "step 4450: train loss 1.3839, val loss 1.6369\n",
      "step 4500: train loss 1.3816, val loss 1.6471\n",
      "step 4550: train loss 1.3787, val loss 1.6537\n",
      "step 4600: train loss 1.3746, val loss 1.6446\n",
      "step 4650: train loss 1.3749, val loss 1.6526\n",
      "step 4700: train loss 1.3664, val loss 1.6542\n",
      "step 4750: train loss 1.3663, val loss 1.6506\n",
      "step 4800: train loss 1.3639, val loss 1.6592\n",
      "step 4850: train loss 1.3617, val loss 1.6482\n",
      "step 4900: train loss 1.3589, val loss 1.6600\n",
      "step 4950: train loss 1.3571, val loss 1.6585\n",
      "step 5000: train loss 1.3558, val loss 1.6576\n",
      "step 5050: train loss 1.3549, val loss 1.6616\n",
      "step 5100: train loss 1.3532, val loss 1.6732\n",
      "step 5150: train loss 1.3487, val loss 1.6614\n",
      "step 5200: train loss 1.3456, val loss 1.6617\n",
      "step 5250: train loss 1.3471, val loss 1.6721\n",
      "step 5300: train loss 1.3395, val loss 1.6707\n",
      "step 5350: train loss 1.3388, val loss 1.6722\n",
      "step 5400: train loss 1.3362, val loss 1.6767\n",
      "step 5450: train loss 1.3309, val loss 1.6771\n",
      "step 5500: train loss 1.3302, val loss 1.6822\n",
      "step 5550: train loss 1.3237, val loss 1.6798\n",
      "step 5600: train loss 1.3207, val loss 1.6871\n",
      "step 5650: train loss 1.3212, val loss 1.6823\n",
      "step 5700: train loss 1.3160, val loss 1.6900\n",
      "step 5750: train loss 1.3191, val loss 1.6727\n",
      "step 5800: train loss 1.3108, val loss 1.7005\n",
      "step 5850: train loss 1.3110, val loss 1.6879\n",
      "step 5900: train loss 1.3033, val loss 1.6901\n",
      "step 5950: train loss 1.3002, val loss 1.6905\n",
      "step 6000: train loss 1.2978, val loss 1.6901\n",
      "step 6050: train loss 1.3010, val loss 1.6870\n",
      "step 6100: train loss 1.3005, val loss 1.6861\n",
      "step 6150: train loss 1.2889, val loss 1.7094\n",
      "step 6200: train loss 1.2872, val loss 1.7001\n",
      "step 6250: train loss 1.2897, val loss 1.7104\n",
      "step 6300: train loss 1.2855, val loss 1.7005\n",
      "step 6350: train loss 1.2861, val loss 1.6957\n",
      "step 6400: train loss 1.2809, val loss 1.7034\n",
      "step 6450: train loss 1.2764, val loss 1.7090\n",
      "step 6500: train loss 1.2734, val loss 1.7202\n",
      "step 6550: train loss 1.2765, val loss 1.7250\n",
      "step 6600: train loss 1.2715, val loss 1.7152\n",
      "step 6650: train loss 1.2685, val loss 1.7172\n",
      "step 6700: train loss 1.2654, val loss 1.7265\n",
      "step 6750: train loss 1.2651, val loss 1.7131\n",
      "step 6800: train loss 1.2622, val loss 1.7337\n",
      "step 6850: train loss 1.2563, val loss 1.7193\n",
      "step 6900: train loss 1.2613, val loss 1.7125\n",
      "step 6950: train loss 1.2560, val loss 1.7202\n",
      "step 7000: train loss 1.2488, val loss 1.7389\n",
      "step 7050: train loss 1.2540, val loss 1.7333\n",
      "step 7100: train loss 1.2483, val loss 1.7389\n",
      "step 7150: train loss 1.2471, val loss 1.7286\n",
      "step 7200: train loss 1.2420, val loss 1.7294\n",
      "step 7250: train loss 1.2408, val loss 1.7455\n",
      "step 7300: train loss 1.2373, val loss 1.7416\n",
      "step 7350: train loss 1.2326, val loss 1.7603\n",
      "step 7400: train loss 1.2315, val loss 1.7523\n",
      "step 7450: train loss 1.2296, val loss 1.7505\n",
      "step 7500: train loss 1.2301, val loss 1.7582\n",
      "step 7550: train loss 1.2250, val loss 1.7521\n",
      "step 7600: train loss 1.2252, val loss 1.7757\n",
      "step 7650: train loss 1.2154, val loss 1.7681\n",
      "step 7700: train loss 1.2209, val loss 1.7642\n",
      "step 7750: train loss 1.2159, val loss 1.7592\n",
      "step 7800: train loss 1.2156, val loss 1.7642\n",
      "step 7850: train loss 1.2073, val loss 1.7770\n",
      "step 7900: train loss 1.2092, val loss 1.7802\n",
      "step 7950: train loss 1.2110, val loss 1.7600\n",
      "step 8000: train loss 1.2005, val loss 1.7826\n",
      "step 8050: train loss 1.2081, val loss 1.7744\n",
      "step 8100: train loss 1.2044, val loss 1.7717\n",
      "step 8150: train loss 1.2023, val loss 1.7718\n",
      "step 8200: train loss 1.1969, val loss 1.7725\n",
      "step 8250: train loss 1.1997, val loss 1.7662\n",
      "step 8300: train loss 1.1923, val loss 1.7838\n",
      "step 8350: train loss 1.1865, val loss 1.7948\n",
      "step 8400: train loss 1.1899, val loss 1.7818\n",
      "step 8450: train loss 1.1875, val loss 1.7843\n",
      "step 8500: train loss 1.1854, val loss 1.7910\n",
      "step 8550: train loss 1.1814, val loss 1.7961\n",
      "step 8600: train loss 1.1795, val loss 1.8114\n",
      "step 8650: train loss 1.1834, val loss 1.7982\n",
      "step 8700: train loss 1.1742, val loss 1.8090\n",
      "step 8750: train loss 1.1728, val loss 1.8136\n",
      "step 8800: train loss 1.1765, val loss 1.7985\n",
      "step 8850: train loss 1.1735, val loss 1.8075\n",
      "step 8900: train loss 1.1681, val loss 1.8069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8950: train loss 1.1666, val loss 1.8135\n",
      "step 9000: train loss 1.1655, val loss 1.8115\n",
      "step 9050: train loss 1.1663, val loss 1.8051\n",
      "step 9100: train loss 1.1706, val loss 1.7941\n",
      "step 9150: train loss 1.1617, val loss 1.8073\n",
      "step 9200: train loss 1.1616, val loss 1.8177\n",
      "step 9250: train loss 1.1630, val loss 1.8125\n",
      "step 9300: train loss 1.1596, val loss 1.8042\n",
      "step 9350: train loss 1.1598, val loss 1.8041\n",
      "step 9400: train loss 1.1495, val loss 1.8378\n",
      "step 9450: train loss 1.1441, val loss 1.8288\n",
      "step 9500: train loss 1.1522, val loss 1.8096\n",
      "step 9550: train loss 1.1473, val loss 1.8316\n",
      "step 9600: train loss 1.1473, val loss 1.8287\n",
      "step 9650: train loss 1.1444, val loss 1.8390\n",
      "step 9700: train loss 1.1425, val loss 1.8398\n",
      "step 9750: train loss 1.1445, val loss 1.8484\n",
      "step 9800: train loss 1.1385, val loss 1.8448\n",
      "step 9850: train loss 1.1377, val loss 1.8386\n",
      "step 9900: train loss 1.1376, val loss 1.8283\n",
      "step 9950: train loss 1.1323, val loss 1.8435\n",
      "step 9999: train loss 1.1303, val loss 1.8539\n",
      "Wall time: 39min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_steps = 10000\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if step % eval_interval == 0 or step == num_steps - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53195195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(m, 'SmallMusicModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c73d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
